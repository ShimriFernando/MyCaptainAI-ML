{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZExnSn6aOl8iv0qXjR1MT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShimriFernando/MyCaptainAI-ML/blob/master/ML_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz9DUTr18DYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52edfca2-2888-4943-f03e-b15c3ee9e4c3"
      },
      "source": [
        "import numpy \n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DLZVT3Q9Cnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open(\"Frankenstein-21.txt\").read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP_sulfnDhtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_words(input):\n",
        "    input = input.lower()\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECvfI9EAFe3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c,i)for i, c in enumerate(chars))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjoHAv2dFsdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "04dd9d3f-a1d8-44d6-b753-d1e1446d33e0"
      },
      "source": [
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print(\"Total number of characters:\", input_len)\n",
        "print(\"Total vocab:\", vocab_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters: 3906\n",
            "Total vocab: 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FovLNEOF2cR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17ed7d3b-58bf-466c-8da8-4816ab722fa7"
      },
      "source": [
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])\n",
        "n_patterns = len(x_data)\n",
        "print(\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 3806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZOBWOiVGMxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvkdvbuXGWYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I3h-jprGbxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSxp9AhmG2aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJe1uddlG8u8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb8x9z9NHBw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a7dd96a-b29b-45de-91bc-511908b16a9c"
      },
      "source": [
        "model.fit(X, y, batch_size=1000, epochs=100, verbose=1, callbacks=desired_callbacks)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3806/3806 [==============================] - 3s 774us/step - loss: 2.8098\n",
            "\n",
            "Epoch 00001: loss improved from 2.81508 to 2.80985, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/100\n",
            "3806/3806 [==============================] - 2s 520us/step - loss: 2.8036\n",
            "\n",
            "Epoch 00002: loss improved from 2.80985 to 2.80357, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/100\n",
            "3806/3806 [==============================] - 2s 533us/step - loss: 2.7957\n",
            "\n",
            "Epoch 00003: loss improved from 2.80357 to 2.79572, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/100\n",
            "3806/3806 [==============================] - 2s 516us/step - loss: 2.7861\n",
            "\n",
            "Epoch 00004: loss improved from 2.79572 to 2.78608, saving model to model_weights_saved.hdf5\n",
            "Epoch 5/100\n",
            "3806/3806 [==============================] - 2s 535us/step - loss: 2.7751\n",
            "\n",
            "Epoch 00005: loss improved from 2.78608 to 2.77515, saving model to model_weights_saved.hdf5\n",
            "Epoch 6/100\n",
            "3806/3806 [==============================] - 2s 519us/step - loss: 2.7799\n",
            "\n",
            "Epoch 00006: loss did not improve from 2.77515\n",
            "Epoch 7/100\n",
            "3806/3806 [==============================] - 2s 531us/step - loss: 2.7699\n",
            "\n",
            "Epoch 00007: loss improved from 2.77515 to 2.76986, saving model to model_weights_saved.hdf5\n",
            "Epoch 8/100\n",
            "3806/3806 [==============================] - 2s 519us/step - loss: 2.7547\n",
            "\n",
            "Epoch 00008: loss improved from 2.76986 to 2.75471, saving model to model_weights_saved.hdf5\n",
            "Epoch 9/100\n",
            "3806/3806 [==============================] - 2s 541us/step - loss: 2.7488\n",
            "\n",
            "Epoch 00009: loss improved from 2.75471 to 2.74876, saving model to model_weights_saved.hdf5\n",
            "Epoch 10/100\n",
            "3806/3806 [==============================] - 2s 535us/step - loss: 2.7449\n",
            "\n",
            "Epoch 00010: loss improved from 2.74876 to 2.74495, saving model to model_weights_saved.hdf5\n",
            "Epoch 11/100\n",
            "3806/3806 [==============================] - 2s 520us/step - loss: 2.7313\n",
            "\n",
            "Epoch 00011: loss improved from 2.74495 to 2.73132, saving model to model_weights_saved.hdf5\n",
            "Epoch 12/100\n",
            "3806/3806 [==============================] - 2s 541us/step - loss: 2.7268\n",
            "\n",
            "Epoch 00012: loss improved from 2.73132 to 2.72684, saving model to model_weights_saved.hdf5\n",
            "Epoch 13/100\n",
            "3806/3806 [==============================] - 2s 536us/step - loss: 2.7128\n",
            "\n",
            "Epoch 00013: loss improved from 2.72684 to 2.71280, saving model to model_weights_saved.hdf5\n",
            "Epoch 14/100\n",
            "3806/3806 [==============================] - 2s 531us/step - loss: 2.7119\n",
            "\n",
            "Epoch 00014: loss improved from 2.71280 to 2.71193, saving model to model_weights_saved.hdf5\n",
            "Epoch 15/100\n",
            "3806/3806 [==============================] - 2s 534us/step - loss: 2.6987\n",
            "\n",
            "Epoch 00015: loss improved from 2.71193 to 2.69874, saving model to model_weights_saved.hdf5\n",
            "Epoch 16/100\n",
            "3806/3806 [==============================] - 2s 534us/step - loss: 2.6844\n",
            "\n",
            "Epoch 00016: loss improved from 2.69874 to 2.68445, saving model to model_weights_saved.hdf5\n",
            "Epoch 17/100\n",
            "3806/3806 [==============================] - 2s 532us/step - loss: 2.6760\n",
            "\n",
            "Epoch 00017: loss improved from 2.68445 to 2.67601, saving model to model_weights_saved.hdf5\n",
            "Epoch 18/100\n",
            "3806/3806 [==============================] - 2s 527us/step - loss: 2.6835\n",
            "\n",
            "Epoch 00018: loss did not improve from 2.67601\n",
            "Epoch 19/100\n",
            "3806/3806 [==============================] - 2s 529us/step - loss: 2.6783\n",
            "\n",
            "Epoch 00019: loss did not improve from 2.67601\n",
            "Epoch 20/100\n",
            "3806/3806 [==============================] - 2s 527us/step - loss: 2.6640\n",
            "\n",
            "Epoch 00020: loss improved from 2.67601 to 2.66402, saving model to model_weights_saved.hdf5\n",
            "Epoch 21/100\n",
            "3806/3806 [==============================] - 2s 519us/step - loss: 2.6584\n",
            "\n",
            "Epoch 00021: loss improved from 2.66402 to 2.65838, saving model to model_weights_saved.hdf5\n",
            "Epoch 22/100\n",
            "3806/3806 [==============================] - 2s 529us/step - loss: 2.6534\n",
            "\n",
            "Epoch 00022: loss improved from 2.65838 to 2.65338, saving model to model_weights_saved.hdf5\n",
            "Epoch 23/100\n",
            "3806/3806 [==============================] - 2s 506us/step - loss: 2.6460\n",
            "\n",
            "Epoch 00023: loss improved from 2.65338 to 2.64601, saving model to model_weights_saved.hdf5\n",
            "Epoch 24/100\n",
            "3806/3806 [==============================] - 2s 523us/step - loss: 2.6329\n",
            "\n",
            "Epoch 00024: loss improved from 2.64601 to 2.63292, saving model to model_weights_saved.hdf5\n",
            "Epoch 25/100\n",
            "3806/3806 [==============================] - 2s 523us/step - loss: 2.6228\n",
            "\n",
            "Epoch 00025: loss improved from 2.63292 to 2.62275, saving model to model_weights_saved.hdf5\n",
            "Epoch 26/100\n",
            "3806/3806 [==============================] - 2s 524us/step - loss: 2.6162\n",
            "\n",
            "Epoch 00026: loss improved from 2.62275 to 2.61617, saving model to model_weights_saved.hdf5\n",
            "Epoch 27/100\n",
            "3806/3806 [==============================] - 2s 523us/step - loss: 2.6162\n",
            "\n",
            "Epoch 00027: loss improved from 2.61617 to 2.61617, saving model to model_weights_saved.hdf5\n",
            "Epoch 28/100\n",
            "3806/3806 [==============================] - 2s 518us/step - loss: 2.6034\n",
            "\n",
            "Epoch 00028: loss improved from 2.61617 to 2.60338, saving model to model_weights_saved.hdf5\n",
            "Epoch 29/100\n",
            "3806/3806 [==============================] - 2s 518us/step - loss: 2.5940\n",
            "\n",
            "Epoch 00029: loss improved from 2.60338 to 2.59398, saving model to model_weights_saved.hdf5\n",
            "Epoch 30/100\n",
            "3806/3806 [==============================] - 2s 518us/step - loss: 2.5804\n",
            "\n",
            "Epoch 00030: loss improved from 2.59398 to 2.58039, saving model to model_weights_saved.hdf5\n",
            "Epoch 31/100\n",
            "3806/3806 [==============================] - 2s 532us/step - loss: 2.5770\n",
            "\n",
            "Epoch 00031: loss improved from 2.58039 to 2.57705, saving model to model_weights_saved.hdf5\n",
            "Epoch 32/100\n",
            "3806/3806 [==============================] - 2s 530us/step - loss: 2.5735\n",
            "\n",
            "Epoch 00032: loss improved from 2.57705 to 2.57354, saving model to model_weights_saved.hdf5\n",
            "Epoch 33/100\n",
            "3806/3806 [==============================] - 2s 536us/step - loss: 2.5671\n",
            "\n",
            "Epoch 00033: loss improved from 2.57354 to 2.56706, saving model to model_weights_saved.hdf5\n",
            "Epoch 34/100\n",
            "3806/3806 [==============================] - 2s 520us/step - loss: 2.5599\n",
            "\n",
            "Epoch 00034: loss improved from 2.56706 to 2.55993, saving model to model_weights_saved.hdf5\n",
            "Epoch 35/100\n",
            "3806/3806 [==============================] - 2s 517us/step - loss: 2.5448\n",
            "\n",
            "Epoch 00035: loss improved from 2.55993 to 2.54476, saving model to model_weights_saved.hdf5\n",
            "Epoch 36/100\n",
            "3806/3806 [==============================] - 2s 522us/step - loss: 2.5288\n",
            "\n",
            "Epoch 00036: loss improved from 2.54476 to 2.52884, saving model to model_weights_saved.hdf5\n",
            "Epoch 37/100\n",
            "3806/3806 [==============================] - 2s 522us/step - loss: 2.5186\n",
            "\n",
            "Epoch 00037: loss improved from 2.52884 to 2.51865, saving model to model_weights_saved.hdf5\n",
            "Epoch 38/100\n",
            "3806/3806 [==============================] - 2s 515us/step - loss: 2.5096\n",
            "\n",
            "Epoch 00038: loss improved from 2.51865 to 2.50959, saving model to model_weights_saved.hdf5\n",
            "Epoch 39/100\n",
            "3806/3806 [==============================] - 2s 518us/step - loss: 2.4977\n",
            "\n",
            "Epoch 00039: loss improved from 2.50959 to 2.49768, saving model to model_weights_saved.hdf5\n",
            "Epoch 40/100\n",
            "3806/3806 [==============================] - 2s 528us/step - loss: 2.4876\n",
            "\n",
            "Epoch 00040: loss improved from 2.49768 to 2.48759, saving model to model_weights_saved.hdf5\n",
            "Epoch 41/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 2.4762\n",
            "\n",
            "Epoch 00041: loss improved from 2.48759 to 2.47619, saving model to model_weights_saved.hdf5\n",
            "Epoch 42/100\n",
            "3806/3806 [==============================] - 2s 524us/step - loss: 2.4619\n",
            "\n",
            "Epoch 00042: loss improved from 2.47619 to 2.46192, saving model to model_weights_saved.hdf5\n",
            "Epoch 43/100\n",
            "3806/3806 [==============================] - 2s 515us/step - loss: 2.4426\n",
            "\n",
            "Epoch 00043: loss improved from 2.46192 to 2.44263, saving model to model_weights_saved.hdf5\n",
            "Epoch 44/100\n",
            "3806/3806 [==============================] - 2s 536us/step - loss: 2.4387\n",
            "\n",
            "Epoch 00044: loss improved from 2.44263 to 2.43875, saving model to model_weights_saved.hdf5\n",
            "Epoch 45/100\n",
            "3806/3806 [==============================] - 2s 515us/step - loss: 2.4228\n",
            "\n",
            "Epoch 00045: loss improved from 2.43875 to 2.42283, saving model to model_weights_saved.hdf5\n",
            "Epoch 46/100\n",
            "3806/3806 [==============================] - 2s 516us/step - loss: 2.4066\n",
            "\n",
            "Epoch 00046: loss improved from 2.42283 to 2.40661, saving model to model_weights_saved.hdf5\n",
            "Epoch 47/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 2.4010\n",
            "\n",
            "Epoch 00047: loss improved from 2.40661 to 2.40103, saving model to model_weights_saved.hdf5\n",
            "Epoch 48/100\n",
            "3806/3806 [==============================] - 2s 514us/step - loss: 2.3888\n",
            "\n",
            "Epoch 00048: loss improved from 2.40103 to 2.38878, saving model to model_weights_saved.hdf5\n",
            "Epoch 49/100\n",
            "3806/3806 [==============================] - 2s 526us/step - loss: 2.3697\n",
            "\n",
            "Epoch 00049: loss improved from 2.38878 to 2.36971, saving model to model_weights_saved.hdf5\n",
            "Epoch 50/100\n",
            "3806/3806 [==============================] - 2s 527us/step - loss: 2.3484\n",
            "\n",
            "Epoch 00050: loss improved from 2.36971 to 2.34835, saving model to model_weights_saved.hdf5\n",
            "Epoch 51/100\n",
            "3806/3806 [==============================] - 2s 531us/step - loss: 2.3368\n",
            "\n",
            "Epoch 00051: loss improved from 2.34835 to 2.33681, saving model to model_weights_saved.hdf5\n",
            "Epoch 52/100\n",
            "3806/3806 [==============================] - 2s 520us/step - loss: 2.3189\n",
            "\n",
            "Epoch 00052: loss improved from 2.33681 to 2.31892, saving model to model_weights_saved.hdf5\n",
            "Epoch 53/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 2.3088\n",
            "\n",
            "Epoch 00053: loss improved from 2.31892 to 2.30880, saving model to model_weights_saved.hdf5\n",
            "Epoch 54/100\n",
            "3806/3806 [==============================] - 2s 519us/step - loss: 2.2918\n",
            "\n",
            "Epoch 00054: loss improved from 2.30880 to 2.29175, saving model to model_weights_saved.hdf5\n",
            "Epoch 55/100\n",
            "3806/3806 [==============================] - 2s 514us/step - loss: 2.2824\n",
            "\n",
            "Epoch 00055: loss improved from 2.29175 to 2.28241, saving model to model_weights_saved.hdf5\n",
            "Epoch 56/100\n",
            "3806/3806 [==============================] - 2s 533us/step - loss: 2.2455\n",
            "\n",
            "Epoch 00056: loss improved from 2.28241 to 2.24548, saving model to model_weights_saved.hdf5\n",
            "Epoch 57/100\n",
            "3806/3806 [==============================] - 2s 515us/step - loss: 2.2467\n",
            "\n",
            "Epoch 00057: loss did not improve from 2.24548\n",
            "Epoch 58/100\n",
            "3806/3806 [==============================] - 2s 522us/step - loss: 2.2365\n",
            "\n",
            "Epoch 00058: loss improved from 2.24548 to 2.23647, saving model to model_weights_saved.hdf5\n",
            "Epoch 59/100\n",
            "3806/3806 [==============================] - 2s 523us/step - loss: 2.2292\n",
            "\n",
            "Epoch 00059: loss improved from 2.23647 to 2.22920, saving model to model_weights_saved.hdf5\n",
            "Epoch 60/100\n",
            "3806/3806 [==============================] - 2s 522us/step - loss: 2.2270\n",
            "\n",
            "Epoch 00060: loss improved from 2.22920 to 2.22702, saving model to model_weights_saved.hdf5\n",
            "Epoch 61/100\n",
            "3806/3806 [==============================] - 2s 533us/step - loss: 2.1877\n",
            "\n",
            "Epoch 00061: loss improved from 2.22702 to 2.18766, saving model to model_weights_saved.hdf5\n",
            "Epoch 62/100\n",
            "3806/3806 [==============================] - 2s 520us/step - loss: 2.1710\n",
            "\n",
            "Epoch 00062: loss improved from 2.18766 to 2.17102, saving model to model_weights_saved.hdf5\n",
            "Epoch 63/100\n",
            "3806/3806 [==============================] - 2s 526us/step - loss: 2.1422\n",
            "\n",
            "Epoch 00063: loss improved from 2.17102 to 2.14216, saving model to model_weights_saved.hdf5\n",
            "Epoch 64/100\n",
            "3806/3806 [==============================] - 2s 526us/step - loss: 2.1128\n",
            "\n",
            "Epoch 00064: loss improved from 2.14216 to 2.11280, saving model to model_weights_saved.hdf5\n",
            "Epoch 65/100\n",
            "3806/3806 [==============================] - 2s 525us/step - loss: 2.0948\n",
            "\n",
            "Epoch 00065: loss improved from 2.11280 to 2.09484, saving model to model_weights_saved.hdf5\n",
            "Epoch 66/100\n",
            "3806/3806 [==============================] - 2s 529us/step - loss: 2.0712\n",
            "\n",
            "Epoch 00066: loss improved from 2.09484 to 2.07116, saving model to model_weights_saved.hdf5\n",
            "Epoch 67/100\n",
            "3806/3806 [==============================] - 2s 533us/step - loss: 2.0840\n",
            "\n",
            "Epoch 00067: loss did not improve from 2.07116\n",
            "Epoch 68/100\n",
            "3806/3806 [==============================] - 2s 517us/step - loss: 2.0656\n",
            "\n",
            "Epoch 00068: loss improved from 2.07116 to 2.06560, saving model to model_weights_saved.hdf5\n",
            "Epoch 69/100\n",
            "3806/3806 [==============================] - 2s 528us/step - loss: 2.0431\n",
            "\n",
            "Epoch 00069: loss improved from 2.06560 to 2.04311, saving model to model_weights_saved.hdf5\n",
            "Epoch 70/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 2.0239\n",
            "\n",
            "Epoch 00070: loss improved from 2.04311 to 2.02390, saving model to model_weights_saved.hdf5\n",
            "Epoch 71/100\n",
            "3806/3806 [==============================] - 2s 525us/step - loss: 1.9947\n",
            "\n",
            "Epoch 00071: loss improved from 2.02390 to 1.99472, saving model to model_weights_saved.hdf5\n",
            "Epoch 72/100\n",
            "3806/3806 [==============================] - 2s 524us/step - loss: 1.9781\n",
            "\n",
            "Epoch 00072: loss improved from 1.99472 to 1.97810, saving model to model_weights_saved.hdf5\n",
            "Epoch 73/100\n",
            "3806/3806 [==============================] - 2s 517us/step - loss: 1.9560\n",
            "\n",
            "Epoch 00073: loss improved from 1.97810 to 1.95596, saving model to model_weights_saved.hdf5\n",
            "Epoch 74/100\n",
            "3806/3806 [==============================] - 2s 513us/step - loss: 1.9464\n",
            "\n",
            "Epoch 00074: loss improved from 1.95596 to 1.94644, saving model to model_weights_saved.hdf5\n",
            "Epoch 75/100\n",
            "3806/3806 [==============================] - 2s 517us/step - loss: 1.9376\n",
            "\n",
            "Epoch 00075: loss improved from 1.94644 to 1.93763, saving model to model_weights_saved.hdf5\n",
            "Epoch 76/100\n",
            "3806/3806 [==============================] - 2s 522us/step - loss: 1.9096\n",
            "\n",
            "Epoch 00076: loss improved from 1.93763 to 1.90964, saving model to model_weights_saved.hdf5\n",
            "Epoch 77/100\n",
            "3806/3806 [==============================] - 2s 525us/step - loss: 1.8716\n",
            "\n",
            "Epoch 00077: loss improved from 1.90964 to 1.87160, saving model to model_weights_saved.hdf5\n",
            "Epoch 78/100\n",
            "3806/3806 [==============================] - 2s 524us/step - loss: 1.8714\n",
            "\n",
            "Epoch 00078: loss improved from 1.87160 to 1.87139, saving model to model_weights_saved.hdf5\n",
            "Epoch 79/100\n",
            "3806/3806 [==============================] - 2s 517us/step - loss: 1.8380\n",
            "\n",
            "Epoch 00079: loss improved from 1.87139 to 1.83796, saving model to model_weights_saved.hdf5\n",
            "Epoch 80/100\n",
            "3806/3806 [==============================] - 2s 531us/step - loss: 1.8184\n",
            "\n",
            "Epoch 00080: loss improved from 1.83796 to 1.81836, saving model to model_weights_saved.hdf5\n",
            "Epoch 81/100\n",
            "3806/3806 [==============================] - 2s 529us/step - loss: 1.7891\n",
            "\n",
            "Epoch 00081: loss improved from 1.81836 to 1.78915, saving model to model_weights_saved.hdf5\n",
            "Epoch 82/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 1.7405\n",
            "\n",
            "Epoch 00082: loss improved from 1.78915 to 1.74055, saving model to model_weights_saved.hdf5\n",
            "Epoch 83/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 1.7412\n",
            "\n",
            "Epoch 00083: loss did not improve from 1.74055\n",
            "Epoch 84/100\n",
            "3806/3806 [==============================] - 2s 514us/step - loss: 1.7428\n",
            "\n",
            "Epoch 00084: loss did not improve from 1.74055\n",
            "Epoch 85/100\n",
            "3806/3806 [==============================] - 2s 526us/step - loss: 1.6959\n",
            "\n",
            "Epoch 00085: loss improved from 1.74055 to 1.69591, saving model to model_weights_saved.hdf5\n",
            "Epoch 86/100\n",
            "3806/3806 [==============================] - 2s 531us/step - loss: 1.7016\n",
            "\n",
            "Epoch 00086: loss did not improve from 1.69591\n",
            "Epoch 87/100\n",
            "3806/3806 [==============================] - 2s 519us/step - loss: 1.6533\n",
            "\n",
            "Epoch 00087: loss improved from 1.69591 to 1.65333, saving model to model_weights_saved.hdf5\n",
            "Epoch 88/100\n",
            "3806/3806 [==============================] - 2s 523us/step - loss: 1.6357\n",
            "\n",
            "Epoch 00088: loss improved from 1.65333 to 1.63567, saving model to model_weights_saved.hdf5\n",
            "Epoch 89/100\n",
            "3806/3806 [==============================] - 2s 524us/step - loss: 1.6187\n",
            "\n",
            "Epoch 00089: loss improved from 1.63567 to 1.61865, saving model to model_weights_saved.hdf5\n",
            "Epoch 90/100\n",
            "3806/3806 [==============================] - 2s 522us/step - loss: 1.5977\n",
            "\n",
            "Epoch 00090: loss improved from 1.61865 to 1.59770, saving model to model_weights_saved.hdf5\n",
            "Epoch 91/100\n",
            "3806/3806 [==============================] - 2s 537us/step - loss: 1.5737\n",
            "\n",
            "Epoch 00091: loss improved from 1.59770 to 1.57368, saving model to model_weights_saved.hdf5\n",
            "Epoch 92/100\n",
            "3806/3806 [==============================] - 2s 521us/step - loss: 1.5403\n",
            "\n",
            "Epoch 00092: loss improved from 1.57368 to 1.54034, saving model to model_weights_saved.hdf5\n",
            "Epoch 93/100\n",
            "3806/3806 [==============================] - 2s 520us/step - loss: 1.5346\n",
            "\n",
            "Epoch 00093: loss improved from 1.54034 to 1.53462, saving model to model_weights_saved.hdf5\n",
            "Epoch 94/100\n",
            "3806/3806 [==============================] - 2s 523us/step - loss: 1.4981\n",
            "\n",
            "Epoch 00094: loss improved from 1.53462 to 1.49815, saving model to model_weights_saved.hdf5\n",
            "Epoch 95/100\n",
            "3806/3806 [==============================] - 2s 539us/step - loss: 1.4890\n",
            "\n",
            "Epoch 00095: loss improved from 1.49815 to 1.48904, saving model to model_weights_saved.hdf5\n",
            "Epoch 96/100\n",
            "3806/3806 [==============================] - 2s 525us/step - loss: 1.4824\n",
            "\n",
            "Epoch 00096: loss improved from 1.48904 to 1.48236, saving model to model_weights_saved.hdf5\n",
            "Epoch 97/100\n",
            "3806/3806 [==============================] - 2s 532us/step - loss: 1.4626\n",
            "\n",
            "Epoch 00097: loss improved from 1.48236 to 1.46258, saving model to model_weights_saved.hdf5\n",
            "Epoch 98/100\n",
            "3806/3806 [==============================] - 2s 527us/step - loss: 1.4647\n",
            "\n",
            "Epoch 00098: loss did not improve from 1.46258\n",
            "Epoch 99/100\n",
            "3806/3806 [==============================] - 2s 519us/step - loss: 1.4166\n",
            "\n",
            "Epoch 00099: loss improved from 1.46258 to 1.41659, saving model to model_weights_saved.hdf5\n",
            "Epoch 100/100\n",
            "3806/3806 [==============================] - 2s 518us/step - loss: 1.3970\n",
            "\n",
            "Epoch 00100: loss improved from 1.41659 to 1.39704, saving model to model_weights_saved.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f429670e630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cupo5OvYHkkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDw0bvPFHrJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_to_char = dict((i, c) for i,c in enumerate(chars))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAwQ2Rf_Jz2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5d0e24df-b8db-431f-fdff-302cada6a478"
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print( \"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\"  occupied duties new situation relinquished many public employments devoted education children eldes \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUSrRKpyKPFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3b58a2fd-22e2-40cb-d6d8-7e41874d98d2"
      },
      "source": [
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x/float(vocab_len)\n",
        "    prediction = model.predict(x, verbose = 0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = num_to_char[index]\n",
        "    seq_in = [num_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "as alssintien satt piarsi ciillint stocicsi fossosinn sessed sennen doied botert pooe bomdated lonthr sar sess cone rendd mond motthr man man reve monte wear connene miaeen lorter aather dete rever ahnce consene manren dnnse teved sears sems mine coccd podten man hens reare creed tereon mans piotiee seaeuser aaupy rirte tife eosliee sever aroorrent pemc ceupirity pase coulived livervad snntent masrene sene tottes massed mnnten sears sersed mnndrad senter sers cirpined siaeen ruuent pouha bouranee sone suppor mine sented lotllat seass counered sivery aettiotien stecetion doild bett roorr pobliety foelt cotloved aare inpery pose sise resenn derol sive momd coodd mond mont persen sespent dlil betlortent mamc posters mime ceuiinee leverad tenter aateer liver coucere  ffcmient soonn cespirie live toouuen matr cotiitien diellas aaslott tessed senned severa pooter tecinnn fesporit nereer lott mitted mond peteral mene reauuas mensened snne settelted mann piverat seme coulived aeutirtent doictm"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}